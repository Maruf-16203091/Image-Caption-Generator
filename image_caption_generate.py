# -*- coding: utf-8 -*-
"""image_caption_generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Bom4maxnG8cm6VuQb37E4xpOMLgz8Q8
"""

import os
import nltk
from collections import Counter

import nltk

# Force remove if corrupted or partially downloaded
import shutil
shutil.rmtree('/root/nltk_data/tokenizers/punkt', ignore_errors=True)

# Redownload the correct package
nltk.download('punkt')

import os
import nltk
from collections import Counter

nltk.download('punkt_tab')


# Download tokenizer once
nltk.download('punkt')

# Mount Google Drive (run only once in Colab)
from google.colab import drive
drive.mount('/content/drive')

# Path to caption file in Google Drive
captions_file = '/content/drive/My Drive/ML_Datasets/archive/captions.txt'

# Read captions into a dictionary: {image_id.jpg: [caption1, caption2, ...]}
captions_dict = {}

# Replace tabs with spaces during reading captions
with open(captions_file, 'r') as f:
    next(f)  # skip header line

    for line in f:
        line = line.strip()
        if not line:
            continue

        parts = line.split(',', 1)
        if len(parts) < 2:
            print(f"Skipping malformed line: {line}")
            continue

        image_id, caption = parts[0].strip(), parts[1].replace('\t', ' ').strip().lower()

        captions_dict.setdefault(image_id, []).append(caption)

# Flatten all captions for vocab building
all_captions = []
for caps in captions_dict.values():
    all_captions.extend(caps)

# Then tokenize normally
tokenized_captions = [nltk.tokenize.word_tokenize(c) for c in all_captions]

# Count word frequencies across all captions
threshold = 5  # Minimum word frequency to keep
word_freq = Counter()
for tokens in tokenized_captions:
    word_freq.update(tokens)

# Build vocabulary of words with freq >= threshold
vocab = [word for word, count in word_freq.items() if count >= threshold]

# Add special tokens at the start
special_tokens = ['<pad>', '<start>', '<end>', '<unk>']
vocab = special_tokens + vocab

# Create mappings for word to index and vice versa
word2idx = {word: idx for idx, word in enumerate(vocab)}
idx2word = {idx: word for word, idx in word2idx.items()}

print(f"Vocabulary size: {len(vocab)}")



import os
import torch
from torch.utils.data import Dataset
from PIL import Image
import nltk

class FlickrDataset(Dataset):
    def __init__(self, root_dir, captions_dict, word2idx, transform=None, max_len=20):
        """
        Args:
            root_dir (str): Path to image folder
            captions_dict (dict): {image_id.jpg: [caption1, caption2, ...]}
            word2idx (dict): vocabulary mapping word->index
            transform (callable, optional): image transformations
            max_len (int): max caption length (including special tokens)
        """
        self.root_dir = root_dir
        self.captions_dict = captions_dict
        self.word2idx = word2idx
        self.transform = transform
        self.max_len = max_len

        # Flatten the dict into a list of (image_id, caption) pairs
        self.items = []
        for img_id, captions in captions_dict.items():
            for caption in captions:
                self.items.append((img_id, caption))

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        img_id, caption = self.items[idx]

        # Load image
        img_path = os.path.join(self.root_dir, img_id)
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image file not found: {img_path}")

        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        # Tokenize caption using nltk
        tokens = nltk.tokenize.word_tokenize(caption.lower())

        # Convert tokens to indices with special tokens
        caption_idx = [self.word2idx.get('<start>')]  # start token
        caption_idx += [self.word2idx.get(token, self.word2idx.get('<unk>')) for token in tokens]
        caption_idx.append(self.word2idx.get('<end>'))  # end token

        # Pad or truncate to max_len
        if len(caption_idx) < self.max_len:
            caption_idx += [self.word2idx.get('<pad>')] * (self.max_len - len(caption_idx))
        else:
            caption_idx = caption_idx[:self.max_len]

        caption_tensor = torch.tensor(caption_idx)

        return image, caption_tensor

from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

import torchvision.models as models
import torch.nn as nn

class EncoderCNN(nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(pretrained=True)
        modules = list(resnet.children())[:-1]  # Remove the classification layer
        self.resnet = nn.Sequential(*modules)
        self.linear = nn.Linear(resnet.fc.in_features, embed_size)
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images).squeeze()
        features = self.linear(features)
        features = self.bn(features)
        return features

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(DecoderRNN, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions):
        embeddings = self.embed(captions[:, :-1])  # Exclude <end> token
        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)
        lstm_out, _ = self.lstm(inputs)
        outputs = self.linear(lstm_out)
        return outputs

    def sample(self, features, max_len=20):
        sampled_ids = []
        inputs = features.unsqueeze(1)
        states = None

        for _ in range(max_len):
            lstm_out, states = self.lstm(inputs, states)
            outputs = self.linear(lstm_out.squeeze(1))
            _, predicted = outputs.max(1)
            sampled_ids.append(predicted.item())
            inputs = self.embed(predicted)
            inputs = inputs.unsqueeze(1)
            if predicted == word2idx['<end>']:
                break
        return sampled_ids

import torch.optim as optim

embed_size = 256
hidden_size = 512
vocab_size = len(vocab)
num_epochs = 10
learning_rate = 1e-3

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

encoder = EncoderCNN(embed_size).to(device)
decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)

criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])
params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())
optimizer = optim.Adam(params, lr=learning_rate)

from torch.utils.data import DataLoader


num_epochs = 10
learning_rate = 1e-3
criterion = torch.nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])
optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)

for epoch in range(num_epochs):
    encoder.train()
    decoder.train()
    total_loss = 0.0

    for images, captions in dataloader:
        images, captions = images.to(device), captions.to(device)

        optimizer.zero_grad()
        features = encoder(images)
        outputs = decoder(features, captions)

        # outputs: (batch, max_len, vocab_size), captions: (batch, max_len)
        outputs = outputs[:, :-1, :].reshape(-1, vocab_size)
        targets = captions[:, 1:].reshape(-1)

        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}")

def generate_caption(image_path):
    encoder.eval()
    decoder.eval()

    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        feature = encoder(image)
        sampled_ids = decoder.sample(feature)

    sampled_caption = []
    for word_id in sampled_ids:
        word = idx2word[word_id]
        if word == '<end>':
            break
        sampled_caption.append(word)

    sentence = ' '.join(sampled_caption)
    return sentence

# Example
print(generate_caption('path_to_some_image.jpg'))